{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w9DQx9RBjAsm",
        "outputId": "16fd1013-c83f-4314-a542-b327be5a52f3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.19.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.14.1)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.74.0)\n",
            "Requirement already satisfied: tensorboard~=2.19.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.19.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.10.0)\n",
            "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.0.2)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.14.0)\n",
            "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.5.3)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.8.3)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.8.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zzqavJMJkTib"
      },
      "outputs": [],
      "source": [
        "# !pip install pandas numpy scikit-learn tensorflow==2.15 imbalanced-learn --quiet\n",
        "\n",
        "import os, random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, callbacks\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sa0yi1Y4kcVl"
      },
      "outputs": [],
      "source": [
        "SEED = 42\n",
        "random.seed(SEED); np.random.seed(SEED); tf.random.set_seed(SEED)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GFToXFj-kipC"
      },
      "outputs": [],
      "source": [
        "WRAPPER_FEATURES = [\n",
        "    \"Src_Port\",\"Dst_Port\",\"Protocol\",\"TotLen_Fwd_Pkts\",\n",
        "    \"Fwd_Pkt_Len_Max\",\"Fwd_Pkt_Len_Min\",\"Fwd_Pkt_Len_Mean\",\"Fwd_Pkt_Len_Std\",\n",
        "    \"Fwd_IAT_Tot\",\"Fwd_IAT_Std\",\"Fwd_IAT_Max\",\"Flow_IAT_Min\",\n",
        "    \"Fwd_PSH_Flags\",\"Fwd_URG_Flags\",\"Bwd_URG_Flags\",\"Fwd_Pkts/s\",\n",
        "    \"FIN_Flag_Cnt\",\"SYN_Flag_Cnt\",\"RST_Flag_Cnt\",\"ACK_Flag_Cnt\",\n",
        "    \"URG_Flag_Cnt\",\"CWE_Flag_Count\",\"ECE_Flag_Cnt\",\"Fwd_Seg_Size_Avg\",\n",
        "    \"Fwd_Byts/b_Avg\",\"Fwd_Pkts/b_Avg\",\"Fwd_Blk_Rate_Avg\",\"Bwd_Byts/b_Avg\",\n",
        "    \"Bwd_Pkts/b_Avg\",\"Bwd_Blk_Rate_Avg\",\"Subflow_Fwd_Byts\",\"Init_Fwd_Win_Byts\",\n",
        "    \"Fwd_Act_Data_Pkts\",\"Fwd_Seg_Size_Min\",\"Active_Mean\",\"Active_Std\",\n",
        "    \"Active_Max\",\"Active_Min\",\"Idle_Std\"\n",
        "]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z9f2mu_rklTB"
      },
      "outputs": [],
      "source": [
        "# CHANGE THIS\n",
        "CSV_PATH = \"/content/ResearchDataSet.csv\"\n",
        "df = pd.read_csv(CSV_PATH)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EbWL0c7sksx2",
        "outputId": "a0ad1966-4b4b-4bc2-888b-17bd63cf9fe1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING: missing wrapper columns (skipped): ['FIN_Flag_Cnt', 'Fwd_Act_Data_Pkts', 'RST_Flag_Cnt', 'SYN_Flag_Cnt']\n"
          ]
        }
      ],
      "source": [
        "# 5.1 choose target\n",
        "# If your file has Sub_Cat and you want strictly \"MITM ARP Spoofing\" vs normal:\n",
        "# df[\"Label\"] = df[\"Sub_Cat\"].astype(str).str.lower().str.contains(\"mitm arp spoofing\").astype(int)\n",
        "# Otherwise use the existing numeric Label at the end of your sheet:\n",
        "y_col = \"Label\"\n",
        "\n",
        "# 5.2 drop columns the paper removes (if present)\n",
        "DROP_COLS = [\"Flow_ID\",\"Src_IP\",\"Dst_IP\",\"Timestamp\",\"Label\",\"Cat\",\"Sub_Cat\"]\n",
        "to_drop_now = [c for c in [\"Flow_ID\",\"Src_IP\",\"Dst_IP\",\"Timestamp\",\"Cat\",\"Sub_Cat\"] if c in df.columns]\n",
        "df.drop(columns=to_drop_now, inplace=True)\n",
        "\n",
        "# 5.3 restrict to wrapper features + target (warn on missing)\n",
        "available = [c for c in WRAPPER_FEATURES if c in df.columns]\n",
        "missing = sorted(set(WRAPPER_FEATURES) - set(available))\n",
        "if missing:\n",
        "    print(\"WARNING: missing wrapper columns (skipped):\", missing)\n",
        "\n",
        "cols = available + [y_col]\n",
        "df = df[cols].drop_duplicates().copy()\n",
        "\n",
        "# 5.4 sanitize & impute\n",
        "df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "for c in available:\n",
        "    if df[c].isna().any():\n",
        "        df[c].fillna(df[c].median(), inplace=True)\n",
        "\n",
        "# 5.5 split & scale (no leakage)\n",
        "X = df[available].astype(float)\n",
        "y = df[y_col].astype(int)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.20, random_state=SEED, stratify=y\n",
        ")\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "X_train_s = scaler.fit_transform(X_train)\n",
        "X_test_s  = scaler.transform(X_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Er0uk3jEk8Ks",
        "outputId": "e5f43973-9a0c-46cf-f30f-735ae83a7342"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{0: 0.7849565032887758, 1: 1.3773268801191363}"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "classes = np.unique(y_train)\n",
        "cw = compute_class_weight(class_weight=\"balanced\", classes=classes, y=y_train)\n",
        "CLASS_WEIGHT = {int(k): float(v) for k,v in zip(classes, cw)}\n",
        "CLASS_WEIGHT\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        },
        "id": "qTOGTHEhlcXk",
        "outputId": "7085796b-3368-4b08-cf75-7881c3110d36"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,608</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_2           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_3           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │         \u001b[38;5;34m4,608\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_2           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │           \u001b[38;5;34m512\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m8,256\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_3           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │           \u001b[38;5;34m256\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_6 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m2,080\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_7 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m33\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">15,745</span> (61.50 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m15,745\u001b[0m (61.50 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">15,361</span> (60.00 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m15,361\u001b[0m (60.00 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">384</span> (1.50 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m384\u001b[0m (1.50 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "INPUT_DIM = X_train_s.shape[1]\n",
        "\n",
        "def make_model(input_dim=INPUT_DIM, dropout=0.2, lr=1e-3):\n",
        "    model = keras.Sequential([\n",
        "        layers.Input(shape=(input_dim,)),\n",
        "        layers.Dense(128, activation=\"relu\"),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Dropout(dropout),\n",
        "\n",
        "        layers.Dense(64, activation=\"relu\"),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Dropout(dropout/2),\n",
        "\n",
        "        layers.Dense(32, activation=\"relu\"),\n",
        "        layers.Dense(1, activation=\"sigmoid\"), \n",
        "    ])\n",
        "    model.compile(\n",
        "        optimizer=keras.optimizers.Adam(learning_rate=lr),\n",
        "        loss=\"binary_crossentropy\",\n",
        "        metrics=[keras.metrics.Precision(name=\"precision\"),\n",
        "                 keras.metrics.Recall(name=\"recall\"),\n",
        "                 keras.metrics.AUC(name=\"auc\"),\n",
        "                 \"accuracy\"]\n",
        "    )\n",
        "    return model\n",
        "\n",
        "model = make_model()\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bLRXM_ITljct",
        "outputId": "14812597-c9c9-4a75-d336-77945106c1c7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 88ms/step - accuracy: 0.6449 - auc: 0.7840 - loss: 0.6453 - precision: 0.5109 - recall: 0.9475 - val_accuracy: 0.7286 - val_auc: 0.9419 - val_loss: 0.6086 - val_precision: 0.9110 - val_recall: 0.2935 - learning_rate: 0.0010\n",
            "Epoch 2/100\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9030 - auc: 0.9572 - loss: 0.2588 - precision: 0.7998 - recall: 0.9720 - val_accuracy: 0.8099 - val_auc: 0.9582 - val_loss: 0.5399 - val_precision: 0.9344 - val_recall: 0.5217 - learning_rate: 0.0010\n",
            "Epoch 3/100\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.9234 - auc: 0.9705 - loss: 0.2019 - precision: 0.8378 - recall: 0.9742 - val_accuracy: 0.8293 - val_auc: 0.9672 - val_loss: 0.5043 - val_precision: 0.9383 - val_recall: 0.5754 - learning_rate: 0.0010\n",
            "Epoch 4/100\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.9326 - auc: 0.9754 - loss: 0.1778 - precision: 0.8558 - recall: 0.9758 - val_accuracy: 0.8959 - val_auc: 0.9715 - val_loss: 0.4731 - val_precision: 0.9382 - val_recall: 0.7688 - learning_rate: 0.0010\n",
            "Epoch 5/100\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.9396 - auc: 0.9775 - loss: 0.1645 - precision: 0.8712 - recall: 0.9751 - val_accuracy: 0.9216 - val_auc: 0.9736 - val_loss: 0.4463 - val_precision: 0.9359 - val_recall: 0.8456 - learning_rate: 0.0010\n",
            "Epoch 6/100\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - accuracy: 0.9434 - auc: 0.9793 - loss: 0.1544 - precision: 0.8789 - recall: 0.9763 - val_accuracy: 0.9236 - val_auc: 0.9747 - val_loss: 0.4238 - val_precision: 0.9351 - val_recall: 0.8523 - learning_rate: 0.0010\n",
            "Epoch 7/100\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.9464 - auc: 0.9807 - loss: 0.1490 - precision: 0.8871 - recall: 0.9739 - val_accuracy: 0.9369 - val_auc: 0.9758 - val_loss: 0.4023 - val_precision: 0.9298 - val_recall: 0.8969 - learning_rate: 0.0010\n",
            "Epoch 8/100\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.9464 - auc: 0.9815 - loss: 0.1441 - precision: 0.8894 - recall: 0.9706 - val_accuracy: 0.9444 - val_auc: 0.9779 - val_loss: 0.3821 - val_precision: 0.9259 - val_recall: 0.9231 - learning_rate: 0.0010\n",
            "Epoch 9/100\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9486 - auc: 0.9826 - loss: 0.1380 - precision: 0.8900 - recall: 0.9767 - val_accuracy: 0.9482 - val_auc: 0.9787 - val_loss: 0.3614 - val_precision: 0.9267 - val_recall: 0.9335 - learning_rate: 0.0010\n",
            "Epoch 10/100\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9500 - auc: 0.9827 - loss: 0.1365 - precision: 0.8958 - recall: 0.9732 - val_accuracy: 0.9516 - val_auc: 0.9792 - val_loss: 0.3388 - val_precision: 0.9183 - val_recall: 0.9536 - learning_rate: 0.0010\n",
            "Epoch 11/100\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9503 - auc: 0.9832 - loss: 0.1345 - precision: 0.8956 - recall: 0.9745 - val_accuracy: 0.9500 - val_auc: 0.9805 - val_loss: 0.3196 - val_precision: 0.9098 - val_recall: 0.9597 - learning_rate: 0.0010\n",
            "Epoch 12/100\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9511 - auc: 0.9847 - loss: 0.1302 - precision: 0.8945 - recall: 0.9785 - val_accuracy: 0.9523 - val_auc: 0.9813 - val_loss: 0.3022 - val_precision: 0.9141 - val_recall: 0.9610 - learning_rate: 0.0010\n",
            "Epoch 13/100\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9535 - auc: 0.9846 - loss: 0.1283 - precision: 0.9035 - recall: 0.9739 - val_accuracy: 0.9536 - val_auc: 0.9819 - val_loss: 0.2817 - val_precision: 0.9106 - val_recall: 0.9695 - learning_rate: 0.0010\n",
            "Epoch 14/100\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9518 - auc: 0.9846 - loss: 0.1277 - precision: 0.8988 - recall: 0.9751 - val_accuracy: 0.9536 - val_auc: 0.9822 - val_loss: 0.2611 - val_precision: 0.9078 - val_recall: 0.9732 - learning_rate: 0.0010\n",
            "Epoch 15/100\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9546 - auc: 0.9856 - loss: 0.1245 - precision: 0.9037 - recall: 0.9770 - val_accuracy: 0.9570 - val_auc: 0.9826 - val_loss: 0.2480 - val_precision: 0.9171 - val_recall: 0.9713 - learning_rate: 0.0010\n",
            "Epoch 16/100\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - accuracy: 0.9548 - auc: 0.9862 - loss: 0.1220 - precision: 0.9043 - recall: 0.9769 - val_accuracy: 0.9570 - val_auc: 0.9832 - val_loss: 0.2288 - val_precision: 0.9175 - val_recall: 0.9707 - learning_rate: 0.0010\n",
            "Epoch 17/100\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.9548 - auc: 0.9865 - loss: 0.1202 - precision: 0.9038 - recall: 0.9776 - val_accuracy: 0.9552 - val_auc: 0.9840 - val_loss: 0.2050 - val_precision: 0.9110 - val_recall: 0.9738 - learning_rate: 0.0010\n",
            "Epoch 18/100\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.9554 - auc: 0.9857 - loss: 0.1216 - precision: 0.9060 - recall: 0.9766 - val_accuracy: 0.9561 - val_auc: 0.9841 - val_loss: 0.2006 - val_precision: 0.9173 - val_recall: 0.9683 - learning_rate: 0.0010\n",
            "Epoch 19/100\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.9546 - auc: 0.9861 - loss: 0.1213 - precision: 0.9035 - recall: 0.9774 - val_accuracy: 0.9581 - val_auc: 0.9844 - val_loss: 0.1854 - val_precision: 0.9173 - val_recall: 0.9744 - learning_rate: 0.0010\n",
            "Epoch 20/100\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.9548 - auc: 0.9858 - loss: 0.1207 - precision: 0.9057 - recall: 0.9750 - val_accuracy: 0.9583 - val_auc: 0.9853 - val_loss: 0.1748 - val_precision: 0.9193 - val_recall: 0.9725 - learning_rate: 0.0010\n",
            "Epoch 21/100\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - accuracy: 0.9545 - auc: 0.9862 - loss: 0.1188 - precision: 0.8996 - recall: 0.9823 - val_accuracy: 0.9574 - val_auc: 0.9857 - val_loss: 0.1627 - val_precision: 0.9148 - val_recall: 0.9756 - learning_rate: 0.0010\n",
            "Epoch 22/100\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - accuracy: 0.9562 - auc: 0.9871 - loss: 0.1162 - precision: 0.9050 - recall: 0.9804 - val_accuracy: 0.9597 - val_auc: 0.9859 - val_loss: 0.1598 - val_precision: 0.9274 - val_recall: 0.9664 - learning_rate: 0.0010\n",
            "Epoch 23/100\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - accuracy: 0.9579 - auc: 0.9869 - loss: 0.1154 - precision: 0.9089 - recall: 0.9805 - val_accuracy: 0.9583 - val_auc: 0.9861 - val_loss: 0.1501 - val_precision: 0.9222 - val_recall: 0.9689 - learning_rate: 0.0010\n",
            "Epoch 24/100\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - accuracy: 0.9554 - auc: 0.9870 - loss: 0.1158 - precision: 0.9047 - recall: 0.9785 - val_accuracy: 0.9586 - val_auc: 0.9862 - val_loss: 0.1435 - val_precision: 0.9232 - val_recall: 0.9683 - learning_rate: 0.0010\n",
            "Epoch 25/100\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9566 - auc: 0.9872 - loss: 0.1144 - precision: 0.9081 - recall: 0.9775 - val_accuracy: 0.9595 - val_auc: 0.9864 - val_loss: 0.1374 - val_precision: 0.9244 - val_recall: 0.9695 - learning_rate: 0.0010\n",
            "Epoch 26/100\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.9580 - auc: 0.9878 - loss: 0.1113 - precision: 0.9091 - recall: 0.9806 - val_accuracy: 0.9604 - val_auc: 0.9869 - val_loss: 0.1348 - val_precision: 0.9246 - val_recall: 0.9719 - learning_rate: 0.0010\n",
            "Epoch 27/100\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9579 - auc: 0.9881 - loss: 0.1121 - precision: 0.9069 - recall: 0.9832 - val_accuracy: 0.9626 - val_auc: 0.9879 - val_loss: 0.1273 - val_precision: 0.9289 - val_recall: 0.9732 - learning_rate: 0.0010\n",
            "Epoch 28/100\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.9591 - auc: 0.9882 - loss: 0.1096 - precision: 0.9101 - recall: 0.9828 - val_accuracy: 0.9617 - val_auc: 0.9876 - val_loss: 0.1235 - val_precision: 0.9258 - val_recall: 0.9744 - learning_rate: 0.0010\n",
            "Epoch 29/100\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9602 - auc: 0.9879 - loss: 0.1087 - precision: 0.9114 - recall: 0.9844 - val_accuracy: 0.9628 - val_auc: 0.9881 - val_loss: 0.1201 - val_precision: 0.9275 - val_recall: 0.9756 - learning_rate: 0.0010\n",
            "Epoch 30/100\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.9577 - auc: 0.9871 - loss: 0.1132 - precision: 0.9072 - recall: 0.9822 - val_accuracy: 0.9601 - val_auc: 0.9881 - val_loss: 0.1186 - val_precision: 0.9192 - val_recall: 0.9780 - learning_rate: 0.0010\n",
            "Epoch 31/100\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - accuracy: 0.9601 - auc: 0.9882 - loss: 0.1076 - precision: 0.9123 - recall: 0.9829 - val_accuracy: 0.9631 - val_auc: 0.9884 - val_loss: 0.1153 - val_precision: 0.9256 - val_recall: 0.9786 - learning_rate: 0.0010\n",
            "Epoch 32/100\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9586 - auc: 0.9881 - loss: 0.1086 - precision: 0.9113 - recall: 0.9795 - val_accuracy: 0.9644 - val_auc: 0.9887 - val_loss: 0.1117 - val_precision: 0.9283 - val_recall: 0.9793 - learning_rate: 0.0010\n",
            "Epoch 33/100\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.9607 - auc: 0.9886 - loss: 0.1070 - precision: 0.9122 - recall: 0.9847 - val_accuracy: 0.9635 - val_auc: 0.9891 - val_loss: 0.1106 - val_precision: 0.9261 - val_recall: 0.9793 - learning_rate: 0.0010\n",
            "Epoch 34/100\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9608 - auc: 0.9888 - loss: 0.1052 - precision: 0.9139 - recall: 0.9830 - val_accuracy: 0.9649 - val_auc: 0.9889 - val_loss: 0.1095 - val_precision: 0.9364 - val_recall: 0.9707 - learning_rate: 0.0010\n",
            "Epoch 35/100\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - accuracy: 0.9597 - auc: 0.9888 - loss: 0.1050 - precision: 0.9111 - recall: 0.9831 - val_accuracy: 0.9660 - val_auc: 0.9887 - val_loss: 0.1102 - val_precision: 0.9326 - val_recall: 0.9786 - learning_rate: 0.0010\n",
            "Epoch 36/100\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.9611 - auc: 0.9887 - loss: 0.1048 - precision: 0.9158 - recall: 0.9815 - val_accuracy: 0.9667 - val_auc: 0.9892 - val_loss: 0.1048 - val_precision: 0.9322 - val_recall: 0.9811 - learning_rate: 0.0010\n",
            "Epoch 37/100\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9635 - auc: 0.9896 - loss: 0.1001 - precision: 0.9191 - recall: 0.9848 - val_accuracy: 0.9687 - val_auc: 0.9895 - val_loss: 0.1047 - val_precision: 0.9350 - val_recall: 0.9835 - learning_rate: 0.0010\n",
            "Epoch 38/100\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9613 - auc: 0.9893 - loss: 0.1032 - precision: 0.9135 - recall: 0.9851 - val_accuracy: 0.9673 - val_auc: 0.9895 - val_loss: 0.1042 - val_precision: 0.9313 - val_recall: 0.9841 - learning_rate: 0.0010\n",
            "Epoch 39/100\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.9628 - auc: 0.9897 - loss: 0.1012 - precision: 0.9168 - recall: 0.9855 - val_accuracy: 0.9680 - val_auc: 0.9898 - val_loss: 0.1005 - val_precision: 0.9329 - val_recall: 0.9841 - learning_rate: 0.0010\n",
            "Epoch 40/100\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - accuracy: 0.9630 - auc: 0.9897 - loss: 0.1002 - precision: 0.9153 - recall: 0.9878 - val_accuracy: 0.9721 - val_auc: 0.9893 - val_loss: 0.1019 - val_precision: 0.9459 - val_recall: 0.9805 - learning_rate: 0.0010\n",
            "Epoch 41/100\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - accuracy: 0.9627 - auc: 0.9896 - loss: 0.0998 - precision: 0.9184 - recall: 0.9830 - val_accuracy: 0.9709 - val_auc: 0.9895 - val_loss: 0.1036 - val_precision: 0.9410 - val_recall: 0.9829 - learning_rate: 0.0010\n",
            "Epoch 42/100\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9643 - auc: 0.9898 - loss: 0.0980 - precision: 0.9209 - recall: 0.9849 - val_accuracy: 0.9727 - val_auc: 0.9903 - val_loss: 0.0987 - val_precision: 0.9433 - val_recall: 0.9854 - learning_rate: 0.0010\n",
            "Epoch 43/100\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9633 - auc: 0.9898 - loss: 0.0986 - precision: 0.9169 - recall: 0.9866 - val_accuracy: 0.9723 - val_auc: 0.9903 - val_loss: 0.0984 - val_precision: 0.9433 - val_recall: 0.9841 - learning_rate: 0.0010\n",
            "Epoch 44/100\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.9659 - auc: 0.9908 - loss: 0.0933 - precision: 0.9230 - recall: 0.9870 - val_accuracy: 0.9757 - val_auc: 0.9904 - val_loss: 0.0983 - val_precision: 0.9448 - val_recall: 0.9921 - learning_rate: 0.0010\n",
            "Epoch 45/100\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 0.9651 - auc: 0.9909 - loss: 0.0930 - precision: 0.9205 - recall: 0.9878 - val_accuracy: 0.9669 - val_auc: 0.9901 - val_loss: 0.1022 - val_precision: 0.9268 - val_recall: 0.9884 - learning_rate: 0.0010\n",
            "Epoch 46/100\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - accuracy: 0.9635 - auc: 0.9906 - loss: 0.0958 - precision: 0.9168 - recall: 0.9875 - val_accuracy: 0.9734 - val_auc: 0.9909 - val_loss: 0.0944 - val_precision: 0.9445 - val_recall: 0.9860 - learning_rate: 5.0000e-04\n",
            "Epoch 47/100\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - accuracy: 0.9694 - auc: 0.9910 - loss: 0.0897 - precision: 0.9307 - recall: 0.9880 - val_accuracy: 0.9732 - val_auc: 0.9912 - val_loss: 0.0945 - val_precision: 0.9429 - val_recall: 0.9872 - learning_rate: 5.0000e-04\n",
            "Epoch 48/100\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - accuracy: 0.9695 - auc: 0.9916 - loss: 0.0869 - precision: 0.9313 - recall: 0.9874 - val_accuracy: 0.9755 - val_auc: 0.9913 - val_loss: 0.0938 - val_precision: 0.9448 - val_recall: 0.9915 - learning_rate: 5.0000e-04\n",
            "Epoch 49/100\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.9674 - auc: 0.9912 - loss: 0.0896 - precision: 0.9252 - recall: 0.9887 - val_accuracy: 0.9750 - val_auc: 0.9915 - val_loss: 0.0940 - val_precision: 0.9442 - val_recall: 0.9908 - learning_rate: 5.0000e-04\n",
            "Epoch 50/100\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9700 - auc: 0.9911 - loss: 0.0882 - precision: 0.9313 - recall: 0.9890 - val_accuracy: 0.9748 - val_auc: 0.9915 - val_loss: 0.0938 - val_precision: 0.9416 - val_recall: 0.9933 - learning_rate: 5.0000e-04\n",
            "Epoch 51/100\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - accuracy: 0.9700 - auc: 0.9909 - loss: 0.0876 - precision: 0.9298 - recall: 0.9910 - val_accuracy: 0.9736 - val_auc: 0.9910 - val_loss: 0.0943 - val_precision: 0.9440 - val_recall: 0.9872 - learning_rate: 5.0000e-04\n",
            "Epoch 52/100\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9701 - auc: 0.9915 - loss: 0.0870 - precision: 0.9320 - recall: 0.9884 - val_accuracy: 0.9734 - val_auc: 0.9918 - val_loss: 0.0932 - val_precision: 0.9434 - val_recall: 0.9872 - learning_rate: 5.0000e-04\n",
            "Epoch 53/100\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - accuracy: 0.9707 - auc: 0.9915 - loss: 0.0852 - precision: 0.9327 - recall: 0.9895 - val_accuracy: 0.9741 - val_auc: 0.9914 - val_loss: 0.0927 - val_precision: 0.9430 - val_recall: 0.9896 - learning_rate: 5.0000e-04\n",
            "Epoch 54/100\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9707 - auc: 0.9912 - loss: 0.0861 - precision: 0.9327 - recall: 0.9895 - val_accuracy: 0.9757 - val_auc: 0.9918 - val_loss: 0.0941 - val_precision: 0.9443 - val_recall: 0.9927 - learning_rate: 5.0000e-04\n",
            "Epoch 55/100\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.9687 - auc: 0.9911 - loss: 0.0879 - precision: 0.9295 - recall: 0.9874 - val_accuracy: 0.9739 - val_auc: 0.9918 - val_loss: 0.0906 - val_precision: 0.9451 - val_recall: 0.9866 - learning_rate: 5.0000e-04\n",
            "Epoch 56/100\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.9715 - auc: 0.9922 - loss: 0.0831 - precision: 0.9359 - recall: 0.9879 - val_accuracy: 0.9745 - val_auc: 0.9919 - val_loss: 0.0890 - val_precision: 0.9457 - val_recall: 0.9878 - learning_rate: 2.5000e-04\n",
            "Epoch 57/100\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9724 - auc: 0.9924 - loss: 0.0812 - precision: 0.9367 - recall: 0.9897 - val_accuracy: 0.9761 - val_auc: 0.9922 - val_loss: 0.0893 - val_precision: 0.9469 - val_recall: 0.9908 - learning_rate: 2.5000e-04\n",
            "Epoch 58/100\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.9712 - auc: 0.9920 - loss: 0.0838 - precision: 0.9349 - recall: 0.9881 - val_accuracy: 0.9757 - val_auc: 0.9921 - val_loss: 0.0885 - val_precision: 0.9458 - val_recall: 0.9908 - learning_rate: 2.5000e-04\n",
            "Epoch 59/100\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9723 - auc: 0.9926 - loss: 0.0816 - precision: 0.9366 - recall: 0.9895 - val_accuracy: 0.9752 - val_auc: 0.9922 - val_loss: 0.0864 - val_precision: 0.9468 - val_recall: 0.9884 - learning_rate: 2.5000e-04\n",
            "Epoch 60/100\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9723 - auc: 0.9921 - loss: 0.0825 - precision: 0.9367 - recall: 0.9894 - val_accuracy: 0.9748 - val_auc: 0.9921 - val_loss: 0.0880 - val_precision: 0.9457 - val_recall: 0.9884 - learning_rate: 2.5000e-04\n",
            "Epoch 61/100\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9720 - auc: 0.9924 - loss: 0.0816 - precision: 0.9376 - recall: 0.9872 - val_accuracy: 0.9761 - val_auc: 0.9923 - val_loss: 0.0888 - val_precision: 0.9459 - val_recall: 0.9921 - learning_rate: 1.2500e-04\n",
            "Epoch 62/100\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.9723 - auc: 0.9923 - loss: 0.0813 - precision: 0.9363 - recall: 0.9901 - val_accuracy: 0.9757 - val_auc: 0.9921 - val_loss: 0.0883 - val_precision: 0.9464 - val_recall: 0.9902 - learning_rate: 1.2500e-04\n",
            "Epoch 63/100\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - accuracy: 0.9726 - auc: 0.9923 - loss: 0.0803 - precision: 0.9381 - recall: 0.9886 - val_accuracy: 0.9761 - val_auc: 0.9921 - val_loss: 0.0883 - val_precision: 0.9469 - val_recall: 0.9908 - learning_rate: 1.2500e-04\n",
            "Epoch 64/100\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.9731 - auc: 0.9917 - loss: 0.0814 - precision: 0.9394 - recall: 0.9885 - val_accuracy: 0.9773 - val_auc: 0.9922 - val_loss: 0.0883 - val_precision: 0.9466 - val_recall: 0.9945 - learning_rate: 1.2500e-04\n",
            "Epoch 65/100\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - accuracy: 0.9739 - auc: 0.9929 - loss: 0.0799 - precision: 0.9396 - recall: 0.9908 - val_accuracy: 0.9764 - val_auc: 0.9923 - val_loss: 0.0873 - val_precision: 0.9470 - val_recall: 0.9915 - learning_rate: 6.2500e-05\n",
            "Epoch 66/100\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.9727 - auc: 0.9926 - loss: 0.0796 - precision: 0.9366 - recall: 0.9906 - val_accuracy: 0.9759 - val_auc: 0.9923 - val_loss: 0.0871 - val_precision: 0.9469 - val_recall: 0.9902 - learning_rate: 6.2500e-05\n",
            "Epoch 67/100\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 39ms/step - accuracy: 0.9737 - auc: 0.9924 - loss: 0.0800 - precision: 0.9417 - recall: 0.9875 - val_accuracy: 0.9766 - val_auc: 0.9925 - val_loss: 0.0872 - val_precision: 0.9475 - val_recall: 0.9915 - learning_rate: 6.2500e-05\n",
            "Epoch 68/100\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - accuracy: 0.9730 - auc: 0.9923 - loss: 0.0791 - precision: 0.9368 - recall: 0.9913 - val_accuracy: 0.9764 - val_auc: 0.9925 - val_loss: 0.0867 - val_precision: 0.9470 - val_recall: 0.9915 - learning_rate: 6.2500e-05\n",
            "Epoch 69/100\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - accuracy: 0.9740 - auc: 0.9922 - loss: 0.0799 - precision: 0.9406 - recall: 0.9898 - val_accuracy: 0.9764 - val_auc: 0.9924 - val_loss: 0.0867 - val_precision: 0.9475 - val_recall: 0.9908 - learning_rate: 6.2500e-05\n",
            "Epoch 70/100\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9731 - auc: 0.9921 - loss: 0.0811 - precision: 0.9389 - recall: 0.9890 - val_accuracy: 0.9761 - val_auc: 0.9923 - val_loss: 0.0867 - val_precision: 0.9475 - val_recall: 0.9902 - learning_rate: 6.2500e-05\n",
            "Epoch 71/100\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9750 - auc: 0.9926 - loss: 0.0767 - precision: 0.9425 - recall: 0.9906 - val_accuracy: 0.9764 - val_auc: 0.9923 - val_loss: 0.0870 - val_precision: 0.9470 - val_recall: 0.9915 - learning_rate: 3.1250e-05\n",
            "Epoch 72/100\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9740 - auc: 0.9927 - loss: 0.0782 - precision: 0.9400 - recall: 0.9904 - val_accuracy: 0.9766 - val_auc: 0.9923 - val_loss: 0.0872 - val_precision: 0.9465 - val_recall: 0.9927 - learning_rate: 3.1250e-05\n",
            "Epoch 73/100\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.9730 - auc: 0.9924 - loss: 0.0799 - precision: 0.9396 - recall: 0.9880 - val_accuracy: 0.9766 - val_auc: 0.9925 - val_loss: 0.0868 - val_precision: 0.9475 - val_recall: 0.9915 - learning_rate: 3.1250e-05\n",
            "Epoch 74/100\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9739 - auc: 0.9920 - loss: 0.0801 - precision: 0.9391 - recall: 0.9914 - val_accuracy: 0.9766 - val_auc: 0.9924 - val_loss: 0.0867 - val_precision: 0.9475 - val_recall: 0.9915 - learning_rate: 1.5625e-05\n",
            "Epoch 75/100\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9740 - auc: 0.9926 - loss: 0.0789 - precision: 0.9411 - recall: 0.9890 - val_accuracy: 0.9766 - val_auc: 0.9924 - val_loss: 0.0867 - val_precision: 0.9475 - val_recall: 0.9915 - learning_rate: 1.5625e-05\n",
            "Epoch 76/100\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 0.9729 - auc: 0.9926 - loss: 0.0793 - precision: 0.9383 - recall: 0.9893 - val_accuracy: 0.9766 - val_auc: 0.9923 - val_loss: 0.0867 - val_precision: 0.9475 - val_recall: 0.9915 - learning_rate: 1.5625e-05\n",
            "Epoch 77/100\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 0.9745 - auc: 0.9929 - loss: 0.0785 - precision: 0.9421 - recall: 0.9895 - val_accuracy: 0.9766 - val_auc: 0.9924 - val_loss: 0.0867 - val_precision: 0.9475 - val_recall: 0.9915 - learning_rate: 1.0000e-05\n",
            "Epoch 78/100\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - accuracy: 0.9737 - auc: 0.9927 - loss: 0.0783 - precision: 0.9385 - recall: 0.9915 - val_accuracy: 0.9766 - val_auc: 0.9923 - val_loss: 0.0867 - val_precision: 0.9475 - val_recall: 0.9915 - learning_rate: 1.0000e-05\n",
            "Epoch 79/100\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - accuracy: 0.9741 - auc: 0.9923 - loss: 0.0792 - precision: 0.9400 - recall: 0.9907 - val_accuracy: 0.9766 - val_auc: 0.9924 - val_loss: 0.0867 - val_precision: 0.9475 - val_recall: 0.9915 - learning_rate: 1.0000e-05\n",
            "Epoch 80/100\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9741 - auc: 0.9923 - loss: 0.0795 - precision: 0.9420 - recall: 0.9884 - val_accuracy: 0.9768 - val_auc: 0.9924 - val_loss: 0.0868 - val_precision: 0.9476 - val_recall: 0.9921 - learning_rate: 1.0000e-05\n",
            "Epoch 81/100\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9739 - auc: 0.9926 - loss: 0.0793 - precision: 0.9401 - recall: 0.9900 - val_accuracy: 0.9770 - val_auc: 0.9924 - val_loss: 0.0869 - val_precision: 0.9476 - val_recall: 0.9927 - learning_rate: 1.0000e-05\n"
          ]
        }
      ],
      "source": [
        "ES = callbacks.EarlyStopping(monitor=\"val_auc\", mode=\"max\", patience=8, restore_best_weights=True)\n",
        "RLROP = callbacks.ReduceLROnPlateau(monitor=\"val_auc\", mode=\"max\", factor=0.5, patience=3, min_lr=1e-5)\n",
        "CKPT = callbacks.ModelCheckpoint(\"best_wrapper_dl.keras\", monitor=\"val_auc\", mode=\"max\", save_best_only=True)\n",
        "\n",
        "history = model.fit(\n",
        "    X_train_s, y_train,\n",
        "    validation_split=0.2,\n",
        "    epochs=100,\n",
        "    batch_size=1024,\n",
        "    class_weight=CLASS_WEIGHT,\n",
        "    callbacks=[ES, RLROP, CKPT],\n",
        "    verbose=1\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Idixxw4wl8gu",
        "outputId": "dcb7e130-6b9e-4cca-8790-9e50a0ea773e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step\n",
            "Confusion Matrix:\n",
            " [[3426  109]\n",
            " [  20 1995]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9942    0.9692    0.9815      3535\n",
            "           1     0.9482    0.9901    0.9687      2015\n",
            "\n",
            "    accuracy                         0.9768      5550\n",
            "   macro avg     0.9712    0.9796    0.9751      5550\n",
            "weighted avg     0.9775    0.9768    0.9769      5550\n",
            "\n",
            "ROC AUC: 0.9922292846087161\n"
          ]
        }
      ],
      "source": [
        "best = keras.models.load_model(\"best_wrapper_dl.keras\")\n",
        "proba = best.predict(X_test_s, batch_size=4096).ravel()\n",
        "pred  = (proba >= 0.5).astype(int)\n",
        "\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, pred))\n",
        "print(classification_report(y_test, pred, digits=4))\n",
        "print(\"ROC AUC:\", roc_auc_score(y_test, proba))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RK_3NyvWm71A",
        "outputId": "3b13d43e-dee0-4df9-9c0e-e408469fa7ae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved: wrapper_minmax_scaler.joblib, best_wrapper_dl.keras\n"
          ]
        }
      ],
      "source": [
        "import joblib\n",
        "joblib.dump(scaler, \"wrapper_minmax_scaler.joblib\")\n",
        "best.save(\"best_wrapper_dl.keras\")\n",
        "print(\"Saved: wrapper_minmax_scaler.joblib, best_wrapper_dl.keras\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qe45R-lvpBoU",
        "outputId": "8d544067-0167-46bd-ac70-209cc58da77a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved: wrapper_minmax_scaler.joblib, best_wrapper_dl.keras\n"
          ]
        }
      ],
      "source": [
        "import joblib\n",
        "joblib.dump(scaler, \"wrapper_minmax_scaler.joblib\")\n",
        "best.save(\"best_wrapper_dl.keras\")\n",
        "print(\"Saved: wrapper_minmax_scaler.joblib, best_wrapper_dl.keras\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BqO_7QYUtP4h",
        "outputId": "a8ac13fa-78ed-4cdc-b91d-b672e4580ef6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:6 out of the last 6 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7f5cd21bfce0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 231ms/step\n",
            "Row 0: Probability=1.0000 -> Predicted=ATTACK\n",
            "Row 1: Probability=0.0000 -> Predicted=NORMAL\n",
            "Row 2: Probability=0.9736 -> Predicted=ATTACK\n",
            "Row 3: Probability=0.0001 -> Predicted=NORMAL\n",
            "Row 4: Probability=0.0032 -> Predicted=NORMAL\n",
            "Row 5: Probability=0.0000 -> Predicted=NORMAL\n",
            "Row 6: Probability=0.0032 -> Predicted=NORMAL\n",
            "Row 7: Probability=0.9736 -> Predicted=ATTACK\n",
            "Row 8: Probability=0.0000 -> Predicted=NORMAL\n",
            "Row 9: Probability=0.0001 -> Predicted=NORMAL\n",
            "Row 10: Probability=0.0032 -> Predicted=NORMAL\n",
            "Row 11: Probability=0.0032 -> Predicted=NORMAL\n",
            "Row 12: Probability=0.9736 -> Predicted=ATTACK\n",
            "Row 13: Probability=0.0001 -> Predicted=NORMAL\n",
            "Row 14: Probability=0.9736 -> Predicted=ATTACK\n",
            "Row 15: Probability=0.0001 -> Predicted=NORMAL\n",
            "Row 16: Probability=1.0000 -> Predicted=ATTACK\n",
            "Row 17: Probability=0.9736 -> Predicted=ATTACK\n",
            "Row 18: Probability=0.0001 -> Predicted=NORMAL\n",
            "Row 19: Probability=0.0001 -> Predicted=NORMAL\n",
            "Row 20: Probability=0.0000 -> Predicted=NORMAL\n",
            "Row 21: Probability=0.9736 -> Predicted=ATTACK\n",
            "Row 22: Probability=0.0032 -> Predicted=NORMAL\n",
            "Row 23: Probability=0.0032 -> Predicted=NORMAL\n",
            "Row 24: Probability=0.0032 -> Predicted=NORMAL\n",
            "Row 25: Probability=0.0001 -> Predicted=NORMAL\n",
            "Row 26: Probability=0.0032 -> Predicted=NORMAL\n",
            "Row 27: Probability=1.0000 -> Predicted=ATTACK\n",
            "Row 28: Probability=0.0001 -> Predicted=NORMAL\n",
            "Row 29: Probability=0.9736 -> Predicted=ATTACK\n",
            "Row 30: Probability=0.0001 -> Predicted=NORMAL\n",
            "Row 31: Probability=0.0000 -> Predicted=NORMAL\n",
            "Row 32: Probability=0.9736 -> Predicted=ATTACK\n",
            "Row 33: Probability=0.0001 -> Predicted=NORMAL\n",
            "Row 34: Probability=0.9736 -> Predicted=ATTACK\n",
            "Row 35: Probability=0.0032 -> Predicted=NORMAL\n",
            "Row 36: Probability=0.0000 -> Predicted=NORMAL\n",
            "Row 37: Probability=0.0032 -> Predicted=NORMAL\n",
            "Row 38: Probability=0.0001 -> Predicted=NORMAL\n",
            "Row 39: Probability=0.0000 -> Predicted=NORMAL\n",
            "Row 40: Probability=0.0001 -> Predicted=NORMAL\n",
            "Row 41: Probability=0.0001 -> Predicted=NORMAL\n",
            "Row 42: Probability=0.0001 -> Predicted=NORMAL\n",
            "Row 43: Probability=0.9982 -> Predicted=ATTACK\n",
            "Row 44: Probability=0.0001 -> Predicted=NORMAL\n",
            "Row 45: Probability=0.0000 -> Predicted=NORMAL\n",
            "Row 46: Probability=0.9736 -> Predicted=ATTACK\n",
            "Row 47: Probability=0.0001 -> Predicted=NORMAL\n",
            "Row 48: Probability=0.0000 -> Predicted=NORMAL\n",
            "Row 49: Probability=0.0001 -> Predicted=NORMAL\n",
            "Row 50: Probability=0.0032 -> Predicted=NORMAL\n",
            "Row 51: Probability=0.0001 -> Predicted=NORMAL\n",
            "Row 52: Probability=0.0001 -> Predicted=NORMAL\n",
            "Row 53: Probability=0.0000 -> Predicted=NORMAL\n",
            "Row 54: Probability=0.0001 -> Predicted=NORMAL\n",
            "Row 55: Probability=0.9736 -> Predicted=ATTACK\n",
            "Row 56: Probability=0.0001 -> Predicted=NORMAL\n",
            "Row 57: Probability=0.0000 -> Predicted=NORMAL\n",
            "Row 58: Probability=0.0032 -> Predicted=NORMAL\n",
            "Row 59: Probability=1.0000 -> Predicted=ATTACK\n",
            "Row 60: Probability=0.0000 -> Predicted=NORMAL\n",
            "Row 61: Probability=0.9736 -> Predicted=ATTACK\n",
            "Row 62: Probability=0.0001 -> Predicted=NORMAL\n",
            "Row 63: Probability=0.0032 -> Predicted=NORMAL\n",
            "Row 64: Probability=0.0001 -> Predicted=NORMAL\n",
            "Row 65: Probability=0.0001 -> Predicted=NORMAL\n",
            "Row 66: Probability=0.0000 -> Predicted=NORMAL\n",
            "Row 67: Probability=0.0001 -> Predicted=NORMAL\n",
            "Row 68: Probability=0.0032 -> Predicted=NORMAL\n",
            "Row 69: Probability=1.0000 -> Predicted=ATTACK\n",
            "Row 70: Probability=0.9736 -> Predicted=ATTACK\n",
            "Row 71: Probability=0.0000 -> Predicted=NORMAL\n",
            "Row 72: Probability=0.0032 -> Predicted=NORMAL\n",
            "Row 73: Probability=0.0001 -> Predicted=NORMAL\n",
            "Row 74: Probability=0.9736 -> Predicted=ATTACK\n",
            "Row 75: Probability=0.0032 -> Predicted=NORMAL\n",
            "Row 76: Probability=1.0000 -> Predicted=ATTACK\n",
            "Row 77: Probability=0.0001 -> Predicted=NORMAL\n",
            "Row 78: Probability=0.9736 -> Predicted=ATTACK\n",
            "Row 79: Probability=0.0032 -> Predicted=NORMAL\n",
            "Row 80: Probability=0.0032 -> Predicted=NORMAL\n",
            "Row 81: Probability=0.9736 -> Predicted=ATTACK\n",
            "Row 82: Probability=0.0001 -> Predicted=NORMAL\n",
            "Row 83: Probability=0.0000 -> Predicted=NORMAL\n",
            "Row 84: Probability=0.0000 -> Predicted=NORMAL\n",
            "Row 85: Probability=0.0000 -> Predicted=NORMAL\n",
            "Row 86: Probability=0.0001 -> Predicted=NORMAL\n",
            "Row 87: Probability=0.0001 -> Predicted=NORMAL\n",
            "Row 88: Probability=0.0032 -> Predicted=NORMAL\n",
            "Row 89: Probability=0.9736 -> Predicted=ATTACK\n",
            "Row 90: Probability=0.9982 -> Predicted=ATTACK\n",
            "Row 91: Probability=0.9736 -> Predicted=ATTACK\n",
            "Row 92: Probability=1.0000 -> Predicted=ATTACK\n",
            "Row 93: Probability=0.9736 -> Predicted=ATTACK\n",
            "Row 94: Probability=0.0001 -> Predicted=NORMAL\n",
            "Row 95: Probability=0.0000 -> Predicted=NORMAL\n",
            "Row 96: Probability=0.9736 -> Predicted=ATTACK\n",
            "Row 97: Probability=0.0032 -> Predicted=NORMAL\n",
            "Row 98: Probability=1.0000 -> Predicted=ATTACK\n",
            "Row 99: Probability=0.0001 -> Predicted=NORMAL\n",
            "Saved -> unseen_with_preds.csv\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import joblib\n",
        "from tensorflow import keras\n",
        "\n",
        "# Load artifacts\n",
        "scaler_path = \"wrapper_minmax_scaler.joblib\"\n",
        "model_path  = \"best_wrapper_dl.keras\"\n",
        "scaler = joblib.load(scaler_path)\n",
        "model  = keras.models.load_model(model_path)\n",
        "\n",
        "# Load unseen CSV\n",
        "unseen_path = \"/content/mitm_normal_dataset_70_30_shuffled.csv\"  # <-- change to your file\n",
        "df_raw = pd.read_csv(unseen_path)\n",
        "\n",
        "# EXACT column order the scaler was fit with\n",
        "expected_cols = list(getattr(scaler, \"feature_names_in_\", []))\n",
        "if not expected_cols:\n",
        "    raise RuntimeError(\"Scaler does not have feature_names_in_. Refit scaler with pandas DataFrame during training.\")\n",
        "\n",
        "# Build X with exact expected columns, adding any missing as safe defaults\n",
        "X = pd.DataFrame(index=df_raw.index, columns=expected_cols, dtype=\"float64\")\n",
        "\n",
        "for i, col in enumerate(expected_cols):\n",
        "    if col in df_raw.columns:\n",
        "        X[col] = df_raw[col]\n",
        "    else:\n",
        "        # Fill truly-missing training columns with the scaler's learned minimum (safe, consistent default)\n",
        "        X[col] = float(scaler.data_min_[i])\n",
        "\n",
        "# Clean & median-impute any NaNs / infs in present columns\n",
        "X.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "for c in X.columns:\n",
        "    if X[c].isna().any():\n",
        "        X[c].fillna(X[c].median(), inplace=True)\n",
        "\n",
        "# Transform & predict\n",
        "X_scaled = scaler.transform(X)\n",
        "proba = model.predict(X_scaled, batch_size=4096).ravel()\n",
        "pred  = (proba >= 0.5).astype(int)\n",
        "\n",
        "# Print results\n",
        "for i, (p, pr) in enumerate(zip(proba, pred)):\n",
        "    print(f\"Row {i}: Probability={p:.4f} -> Predicted={'ATTACK' if pr==1 else 'NORMAL'}\")\n",
        "\n",
        "# (Optional) save with predictions\n",
        "out = df_raw.copy()\n",
        "out[\"mitm_proba\"] = proba\n",
        "out[\"mitm_pred\"]  = pred\n",
        "out.to_csv(\"unseen_with_preds.csv\", index=False)\n",
        "print(\"Saved -> unseen_with_preds.csv\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
